{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  10 - Learning Policy on Image + Attributes - loss 2\n",
    "\n",
    "Fully connected network with:\n",
    "- Input: \n",
    "    - **184** size vector with:\n",
    "        * client information: [current latitude, current longitud, destination latitude, destination longitude]\n",
    "        * vehicles information: [load, queue, current latitude, current longitud, next latitude, next longitude]\n",
    "    - Image with 31 channel: \n",
    "        * Channel 0 --> client representation. its current location is marked as 1; its destination as -1\n",
    "        * Channels > 0 --> vehicle current location (1)\n",
    "        \n",
    "- Output: One hot encoding vector with 30 entries representing the available vehicles\n",
    "\n",
    "\n",
    "The attribute input is merged later with a partial output of the network\n",
    "\n",
    "loss 1: vehicle assignment\n",
    "loss 2: vehicle position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))*(im_size - 3*(self.cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size, simple, weighted):\n",
    "    \n",
    "    #model.train()\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    #model.eval()\n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, test_y_aux) + (1-weighted)*criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(1,108))\n",
    "b = a[:int(len(a)*0.6)]\n",
    "c = a[int(len(a)*0.6):int(len(a)*0.9)]\n",
    "d = a[int(len(a)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4917)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=40\n",
    "\n",
    "tables = list(range(1,108))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "            tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.9)], \\\n",
    "            tables[int(len(tables)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size, model, simple=False, weighted=0.5, n_epochs=50):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net 1. - W=0.999; 500units; kernel size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc, total_loss = [], []\n",
    "kernel_size = 3\n",
    "for im_size in [30, 50, 10]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = Net_Final(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.999)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "#torch.save(model, 'model_try3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 1.297 Accuracy: 0.858 Test Loss: 4.739 Accuracy: 0.252\n",
      "Average time per epoch 29.022s +- 1.135\n",
      "Max accuracy of 0.256 achieved at epoch 30\n",
      "Epoch 40. Train Loss: 2.060 Accuracy: 0.854 Test Loss: 5.507 Accuracy: 0.256\n",
      "Average time per epoch 28.594s +- 1.173\n",
      "Max accuracy of 0.262 achieved at epoch 31\n",
      "Epoch 40. Train Loss: 1.536 Accuracy: 0.851 Test Loss: 4.928 Accuracy: 0.252\n",
      "Average time per epoch 28.033s +- 0.288\n",
      "Max accuracy of 0.262 achieved at epoch 27\n",
      "Epoch 40. Train Loss: 1.328 Accuracy: 0.860 Test Loss: 4.625 Accuracy: 0.255\n",
      "Average time per epoch 28.025s +- 0.196\n",
      "Max accuracy of 0.260 achieved at epoch 26\n",
      "Epoch 40. Train Loss: 1.482 Accuracy: 0.855 Test Loss: 5.052 Accuracy: 0.253\n",
      "Average time per epoch 28.073s +- 0.244\n",
      "Max accuracy of 0.263 achieved at epoch 36\n",
      "Average accuracy 0.261 +- 0.002. Av loss 4.241\n",
      " -------------\n",
      "Epoch 40. Train Loss: 6.568 Accuracy: 0.794 Test Loss: 4.640 Accuracy: 0.241\n",
      "Average time per epoch 3.352s +- 0.050\n",
      "Max accuracy of 0.259 achieved at epoch 11\n",
      "Epoch 40. Train Loss: 6.663 Accuracy: 0.794 Test Loss: 4.600 Accuracy: 0.249\n",
      "Average time per epoch 3.369s +- 0.062\n",
      "Max accuracy of 0.252 achieved at epoch 34\n",
      "Epoch 40. Train Loss: 6.901 Accuracy: 0.800 Test Loss: 4.518 Accuracy: 0.251\n",
      "Average time per epoch 3.359s +- 0.048\n",
      "Max accuracy of 0.261 achieved at epoch 25\n",
      "Epoch 40. Train Loss: 6.901 Accuracy: 0.801 Test Loss: 4.470 Accuracy: 0.250\n",
      "Average time per epoch 3.367s +- 0.070\n",
      "Max accuracy of 0.259 achieved at epoch 22\n",
      "Epoch 40. Train Loss: 6.934 Accuracy: 0.794 Test Loss: 4.479 Accuracy: 0.253\n",
      "Average time per epoch 3.353s +- 0.049\n",
      "Max accuracy of 0.260 achieved at epoch 28\n",
      "Average accuracy 0.258 +- 0.003. Av loss 4.287\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "kernel_size = 3\n",
    "for im_size in [50, 10]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = Net_Final(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.999)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 4.349 Accuracy: 0.429 Test Loss: 4.468 Accuracy: 0.215\n",
      "Average time per epoch 25.995s +- 0.982\n",
      "Max accuracy of 0.215 achieved at epoch 9\n"
     ]
    }
   ],
   "source": [
    "model_size50 = Net_Final(31, MAX_V, 50, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(50, model_size50, weighted=0.999, n_epochs=10)\n",
    "torch.save(model_size50, 'model_size_50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 5.237 Accuracy: 0.405 Test Loss: 4.512 Accuracy: 0.209\n",
      "Average time per epoch 10.527s +- 0.920\n",
      "Max accuracy of 0.211 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "model_size30 = Net_Final(31, MAX_V, 30, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(30, model_size30, weighted=0.999, n_epochs=10)\n",
    "torch.save(model_size30, 'model_size_30.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImagesP3(31, MAX_V, im_size, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL WITHOUT INTERMEDIATE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4917)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=40\n",
    "\n",
    "tables = list(range(1,98))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "            tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.9)], \\\n",
    "            tables[int(len(tables)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 91.575 Accuracy: 0.935 Test Loss: 188.138 Accuracy: 0.1534\n",
      "Average time per epoch 11.155s +- 1.115\n",
      "Max accuracy of 0.186 achieved at epoch 4\n",
      "Epoch 40. Train Loss: 93.962 Accuracy: 0.931 Test Loss: 186.689 Accuracy: 0.1561\n",
      "Average time per epoch 10.902s +- 0.221\n",
      "Max accuracy of 0.198 achieved at epoch 3\n",
      "Epoch 40. Train Loss: 94.900 Accuracy: 0.931 Test Loss: 188.080 Accuracy: 0.1562\n",
      "Average time per epoch 11.259s +- 0.636\n",
      "Max accuracy of 0.184 achieved at epoch 3\n",
      "Epoch 40. Train Loss: 89.395 Accuracy: 0.944 Test Loss: 190.905 Accuracy: 0.1340\n",
      "Average time per epoch 11.936s +- 1.466\n",
      "Max accuracy of 0.188 achieved at epoch 5\n",
      "Epoch 40. Train Loss: 87.775 Accuracy: 0.943 Test Loss: 189.061 Accuracy: 0.1498\n",
      "Average time per epoch 12.722s +- 1.200\n",
      "Max accuracy of 0.185 achieved at epoch 7\n",
      "Average accuracy 0.188 +- 0.005. Av loss 179.817\n",
      " -------------\n",
      "Epoch 40. Train Loss: 75.894 Accuracy: 0.965 Test Loss: 190.135 Accuracy: 0.1394\n",
      "Average time per epoch 28.978s +- 1.511\n",
      "Max accuracy of 0.171 achieved at epoch 3\n",
      "Epoch 40. Train Loss: 76.580 Accuracy: 0.964 Test Loss: 189.770 Accuracy: 0.1440\n",
      "Average time per epoch 28.358s +- 1.323\n",
      "Max accuracy of 0.181 achieved at epoch 2\n",
      "Epoch 40. Train Loss: 76.139 Accuracy: 0.961 Test Loss: 187.323 Accuracy: 0.1509\n",
      "Average time per epoch 28.183s +- 1.452\n",
      "Max accuracy of 0.172 achieved at epoch 4\n",
      "Epoch 40. Train Loss: 76.500 Accuracy: 0.963 Test Loss: 189.782 Accuracy: 0.1453\n",
      "Average time per epoch 28.628s +- 1.700\n",
      "Max accuracy of 0.175 achieved at epoch 4\n",
      "Epoch 40. Train Loss: 76.000 Accuracy: 0.960 Test Loss: 188.148 Accuracy: 0.1466\n",
      "Average time per epoch 28.947s +- 2.331\n",
      "Max accuracy of 0.171 achieved at epoch 5\n",
      "Average accuracy 0.174 +- 0.004. Av loss 181.861\n",
      " -------------\n",
      "Epoch 40. Train Loss: 147.659 Accuracy: 0.834 Test Loss: 182.246 Accuracy: 0.167\n",
      "Average time per epoch 3.408s +- 0.115\n",
      "Max accuracy of 0.223 achieved at epoch 7\n",
      "Epoch 40. Train Loss: 142.890 Accuracy: 0.845 Test Loss: 183.312 Accuracy: 0.162\n",
      "Average time per epoch 3.434s +- 0.240\n",
      "Max accuracy of 0.224 achieved at epoch 10\n",
      "Epoch 40. Train Loss: 143.699 Accuracy: 0.847 Test Loss: 182.910 Accuracy: 0.161\n",
      "Average time per epoch 3.411s +- 0.276\n",
      "Max accuracy of 0.206 achieved at epoch 8\n",
      "Epoch 40. Train Loss: 145.088 Accuracy: 0.844 Test Loss: 183.175 Accuracy: 0.162\n",
      "Average time per epoch 3.537s +- 0.245\n",
      "Max accuracy of 0.205 achieved at epoch 5\n",
      "Epoch 40. Train Loss: 144.302 Accuracy: 0.849 Test Loss: 183.186 Accuracy: 0.163\n",
      "Average time per epoch 3.421s +- 0.252\n",
      "Max accuracy of 0.210 achieved at epoch 6\n",
      "Average accuracy 0.213 +- 0.008. Av loss 176.302\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "kernel_size = 3\n",
    "for im_size in [30, 50, 10]: # select im size based on previous results \n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = Net_Final(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.001)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4917)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=40\n",
    "\n",
    "tables = list(range(1,98))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "            tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.9)], \\\n",
    "            tables[int(len(tables)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size, simple, weighted):\n",
    "    \n",
    "    #model.train()\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    #model.eval()\n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, test_y_aux) + (1-weighted)*criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 284.910 Accuracy: 0.451 Test Loss: 177.152 Accuracy: 0.207\n",
      "Average time per epoch 3.236s +- 0.199\n",
      "Max accuracy of 0.207 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "model = Net_Final(31, MAX_V, 10, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(10, model, weighted=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'without_loss_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUTURE LOCATION AS -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesVal2(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))*(im_size - 3*(self.cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 300),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(300, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1]\n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train2(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size=30, simple=False, weighted=0.5):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv*2+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][2*i-1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][2*i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv*2+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][2*i -1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][2*i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = criterion1(output1, test_y_aux) + criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "\n",
    "def evaluating_model(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train2(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 3.150 Accuracy: 0.766 Test Loss: 184.683 Accuracy: 0.180\n",
      "Average time per epoch 16.338s +- 0.846\n",
      "Max accuracy of 0.209 achieved at epoch 7\n",
      "Epoch 40. Train Loss: 4.105 Accuracy: 0.760 Test Loss: 184.299 Accuracy: 0.184\n",
      "Average time per epoch 16.223s +- 1.065\n",
      "Max accuracy of 0.208 achieved at epoch 16\n",
      "Epoch 40. Train Loss: 4.590 Accuracy: 0.746 Test Loss: 183.821 Accuracy: 0.178\n",
      "Average time per epoch 16.311s +- 0.959\n",
      "Max accuracy of 0.216 achieved at epoch 11\n",
      "Epoch 40. Train Loss: 3.596 Accuracy: 0.772 Test Loss: 187.088 Accuracy: 0.174\n",
      "Average time per epoch 16.201s +- 1.585\n",
      "Max accuracy of 0.215 achieved at epoch 10\n",
      "Epoch 40. Train Loss: 3.589 Accuracy: 0.765 Test Loss: 186.722 Accuracy: 0.173\n",
      "Average time per epoch 15.540s +- 0.900\n",
      "Max accuracy of 0.208 achieved at epoch 11\n",
      "Average accuracy 0.211 +- 0.004. Av loss 180.079\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesVal2(2*MAX_V+1, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(20):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train2(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Train Loss: 5.502 Accuracy: 0.544 Test Loss: 181.360 Accuracy: 0.200\n",
      "Average time per epoch 15.509s +- 1.044\n",
      "Max accuracy of 0.216 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "model = ImagesVal2(2*MAX_V+1, MAX_V, im_size, kernel_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'clipped_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH CLIPPED VALUES IN IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train3(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size=30, simple=False, weighted=0.5):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = criterion1(output1, test_y_aux) + criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "\n",
    "def evaluating_model3(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train3(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 4.269 Accuracy: 0.764 Test Loss: 187.326 Accuracy: 0.172\n",
      "Average time per epoch 6.149s +- 0.467\n",
      "Max accuracy of 0.198 achieved at epoch 14\n",
      "Epoch 40. Train Loss: 3.888 Accuracy: 0.774 Test Loss: 185.226 Accuracy: 0.186\n",
      "Average time per epoch 6.125s +- 0.214\n",
      "Max accuracy of 0.215 achieved at epoch 12\n",
      "Epoch 40. Train Loss: 3.925 Accuracy: 0.768 Test Loss: 186.414 Accuracy: 0.161\n",
      "Average time per epoch 6.203s +- 0.374\n",
      "Max accuracy of 0.205 achieved at epoch 13\n",
      "Epoch 40. Train Loss: 3.932 Accuracy: 0.778 Test Loss: 184.613 Accuracy: 0.181\n",
      "Average time per epoch 6.170s +- 0.250\n",
      "Max accuracy of 0.206 achieved at epoch 11\n",
      "Epoch 40. Train Loss: 4.150 Accuracy: 0.756 Test Loss: 185.620 Accuracy: 0.166\n",
      "Average time per epoch 6.120s +- 0.203\n",
      "Max accuracy of 0.206 achieved at epoch 11\n",
      "Average accuracy 0.206 +- 0.005. Av loss 180.918\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesVal2(2, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model3(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model3(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(15):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train3(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15. Train Loss: 6.489 Accuracy: 0.470 Test Loss: 182.214 Accuracy: 0.196\n",
      "Average time per epoch 5.938s +- 0.133\n",
      "Max accuracy of 0.196 achieved at epoch 14\n"
     ]
    }
   ],
   "source": [
    "model = ImagesVal2(2, MAX_V, im_size, kernel_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model3(im_size, model, weighted=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'two_channel_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
