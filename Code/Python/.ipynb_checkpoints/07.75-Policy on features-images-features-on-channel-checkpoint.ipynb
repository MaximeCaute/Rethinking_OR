{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  07.7 - Learning Policy on Image + Attributes + CHANGE! \n",
    "\n",
    "Fully connected network with:\n",
    "- Input: \n",
    "    - **184** size vector with:\n",
    "        * client information: [current latitude, current longitud, destination latitude, destination longitude]\n",
    "        * vehicles information: [load, queue, current latitude, current longitud, next latitude, next longitude]\n",
    "    - Image with 31 channel: \n",
    "        * Channel 0 --> client representation. its current location is marked as 1; its destination as -1\n",
    "        * Channels > 0 --> vehicle current location (1)\n",
    "        \n",
    "- Output: One hot encoding vector with 30 entries representing the available vehicles\n",
    "\n",
    "\n",
    "The attribute input is merged later with a partial output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesP2(nn.Module):\n",
    "    def __init__(self, inp, im_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 10\n",
    "        self.cs = 5\n",
    "        self.im_size = im_size\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))*(im_size - 3*(self.cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, inp-1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "                \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = x3.view(-1,2*(self.im_size - 3*(self.cs-1))*(self.im_size - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion, e, im_size=30):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            \n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            x_aux = []\n",
    "            \n",
    "            # iterate along clients\n",
    "            for k, cl in enumerate(t_idx):\n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "                \n",
    "                # iterate along vehicles\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    #x[k][i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "\n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            # output1, output2 = model(train_x, train_x_aux)\n",
    "            # batch_loss = criterion(output1, train_y) + criterion(output2, train_y)\n",
    "            output1= model(train_x)\n",
    "            batch_loss = criterion(output1, train_y)\n",
    "            \n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output1,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "            \n",
    "            #acc.append(100*output.detach().max(1)[1].eq(train_y).sum().item()/(train_y.shape[0]*train_y.shape[1]*train_y.shape[2]))\n",
    "\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            \n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            output1= model(test_x)\n",
    "            batch_loss = criterion(output1, test_y) \n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output1,1)\n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    #print('Epoch {}. Test Loss: {:.3f}'.format(e+1,test_loss))\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures\n",
    "    #return sum_loss, np.mean(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4915)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=50\n",
    "\n",
    "tables = list(range(1,90))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.85)], tables[int(len(tables)*0.85):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size):\n",
    "    model = ImagesP2(31, im_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "        epoch_train(model, train_tables, test_tables, optimizer, criterion, epoch, im_size=im_size)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times\n",
    "\n",
    "\n",
    "def visualize_results(loss, test_loss, acc, test_acc, idx_f):\n",
    "    f, ax = plt.subplots(2, 1, figsize=(20,8))\n",
    "    ax[0].plot(loss)\n",
    "    ax[0].plot(test_loss)\n",
    "    ax[1].plot(acc)\n",
    "    ax[1].plot(test_acc)\n",
    "    plt.show()\n",
    "    \n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc) \n",
    "    iter_max = iter_max[0][0]\n",
    "\n",
    "    fig, ax = plt.subplots(1,5, figsize=(20,4))\n",
    "    k = 0\n",
    "    idx = idx_f[:iter_max+1]\n",
    "    for j, i in enumerate(idx[::-1]):\n",
    "        if len(i) > 0:\n",
    "            ax[k].hist(i, bins=50)\n",
    "            ax[k].set_xlabel('Client')\n",
    "            ax[k].set_ylabel('Num errors')\n",
    "            ax[k].set_title('Iteration {}\\nwith acc:{:.3f}'.format(-j + iter_max, test_acc[-j + iter_max]))\n",
    "            k += 1\n",
    "        if k == 5:\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## values : queue and future queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 75.054 Accuracy: 0.826 Test Loss: 202.577 Accuracy: 0.1139\n",
      "Average time per epoch 12.116s +- 0.450\n",
      "Max accuracy of 0.125 achieved at epoch 14\n"
     ]
    }
   ],
   "source": [
    "loss, acc, test_loss, test_acc, idx_f, times= evaluating_model(30)\n",
    "#visualize_results(loss, test_loss, acc, test_acc, idx_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 at current loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 72.560 Accuracy: 0.835 Test Loss: 239.590 Accuracy: 0.0726\n",
      "Average time per epoch 15.027s +- 42.127\n",
      "Max accuracy of 0.089 achieved at epoch 2\n",
      "Epoch 50. Train Loss: 73.898 Accuracy: 0.830 Test Loss: 222.341 Accuracy: 0.0734\n",
      "Average time per epoch 14.001s +- 0.970\n",
      "Max accuracy of 0.090 achieved at epoch 6\n",
      "Epoch 50. Train Loss: 73.812 Accuracy: 0.824 Test Loss: 235.787 Accuracy: 0.0778\n",
      "Average time per epoch 13.645s +- 0.246\n",
      "Max accuracy of 0.105 achieved at epoch 13\n",
      "Epoch 50. Train Loss: 71.065 Accuracy: 0.835 Test Loss: 238.607 Accuracy: 0.0950\n",
      "Average time per epoch 13.578s +- 0.141\n",
      "Max accuracy of 0.100 achieved at epoch 37\n",
      "Epoch 50. Train Loss: 72.441 Accuracy: 0.827 Test Loss: 222.577 Accuracy: 0.0770\n",
      "Average time per epoch 12.489s +- 1.977\n",
      "Max accuracy of 0.109 achieved at epoch 5\n",
      "Average accuracy 0.099 +- 0.008. Av loss 134.522\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        loss, acc, test_loss, test_acc, idx_f, times = evaluating_model(im_size)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 at current loc and -1 at future loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 66.622 Accuracy: 0.856 Test Loss: 200.499 Accuracy: 0.1138\n",
      "Average time per epoch 11.939s +- 0.341\n",
      "Max accuracy of 0.145 achieved at epoch 14\n"
     ]
    }
   ],
   "source": [
    "loss, acc, test_loss, test_acc, idx_f, times= evaluating_model(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 12.437 Accuracy: 0.970 Test Loss: 252.249 Accuracy: 0.1148\n",
      "Average time per epoch 32.121s +- 0.373\n",
      "Max accuracy of 0.136 achieved at epoch 10\n"
     ]
    }
   ],
   "source": [
    "loss, acc, test_loss, test_acc, idx_f, times= evaluating_model(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 61.745 Accuracy: 0.866 Test Loss: 199.970 Accuracy: 0.1374\n",
      "Average time per epoch 11.753s +- 0.268\n",
      "Max accuracy of 0.141 achieved at epoch 9\n",
      "Epoch 50. Train Loss: 54.710 Accuracy: 0.894 Test Loss: 202.787 Accuracy: 0.1303\n",
      "Average time per epoch 11.793s +- 0.159\n",
      "Max accuracy of 0.139 achieved at epoch 23\n",
      "Epoch 50. Train Loss: 58.578 Accuracy: 0.879 Test Loss: 205.862 Accuracy: 0.1180\n",
      "Average time per epoch 11.886s +- 0.142\n",
      "Max accuracy of 0.142 achieved at epoch 12\n",
      "Epoch 50. Train Loss: 59.620 Accuracy: 0.878 Test Loss: 199.873 Accuracy: 0.1319\n",
      "Average time per epoch 11.969s +- 0.161\n",
      "Max accuracy of 0.154 achieved at epoch 18\n",
      "Epoch 50. Train Loss: 62.110 Accuracy: 0.873 Test Loss: 200.032 Accuracy: 0.1180\n",
      "Average time per epoch 12.005s +- 0.126\n",
      "Max accuracy of 0.145 achieved at epoch 29\n",
      "Average accuracy 0.144 +- 0.005. Av loss 115.797\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        loss, acc, test_loss, test_acc, idx_f, times = evaluating_model(im_size)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
