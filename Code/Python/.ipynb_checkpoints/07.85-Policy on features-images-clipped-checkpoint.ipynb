{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  07.85 - Learning Policy on Image + Attributes - loss 2\n",
    "\n",
    "Fully connected network with:\n",
    "- Input: \n",
    "    - **184** size vector with:\n",
    "        * client information: [current latitude, current longitud, destination latitude, destination longitude]\n",
    "        * vehicles information: [load, queue, current latitude, current longitud, next latitude, next longitude]\n",
    "    - Image with 31 channel: \n",
    "        * Channel 0 --> client representation. its current location is marked as 1; its destination as -1\n",
    "        * Channels > 0 --> vehicle current location (1)\n",
    "        \n",
    "- Output: One hot encoding vector with 30 entries representing the available vehicles\n",
    "\n",
    "\n",
    "The attribute input is merged later with a partial output of the network\n",
    "\n",
    "loss 1: vehicle assignment\n",
    "loss 2: vehicle position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesP3(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 10\n",
    "        cs = 5\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(cs-1))*(im_size - 3*(cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(100, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        cs = 5\n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(cs-1))*(x.shape[-1] - 3*(cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size=30, simple=False, weighted=0.5):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                # all vehicles in channel 1\n",
    "                for i in range(1,31):\n",
    "                    x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = criterion1(output1, test_y_aux) + criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4915)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=50\n",
    "\n",
    "tables = list(range(1,90))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "            tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.9)], \\\n",
    "            tables[int(len(tables)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImagesP3(2, MAX_V, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of parameters: 4123549\n",
      "Num of trainable parameters: 4123549\n"
     ]
    }
   ],
   "source": [
    "print('Total num of parameters: {}'.format(sum(p.numel() for p in model.parameters())))\n",
    "print('Num of trainable parameters: {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size, simple=False, weighted=0.5):\n",
    "    model = ImagesP3(2, MAX_V, im_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2\n",
    "\n",
    "\n",
    "def visualize_results(loss, test_loss, acc, test_acc, idx_f):\n",
    "    f, ax = plt.subplots(2, 1, figsize=(20,8))\n",
    "    ax[0].plot(loss)\n",
    "    ax[0].plot(test_loss)\n",
    "    ax[1].plot(acc)\n",
    "    ax[1].plot(test_acc)\n",
    "    plt.show()\n",
    "    \n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc) \n",
    "    iter_max = iter_max[0][0]\n",
    "\n",
    "    fig, ax = plt.subplots(1,5, figsize=(20,4))\n",
    "    k = 0\n",
    "    idx = idx_f[:iter_max+1]\n",
    "    for j, i in enumerate(idx[::-1]):\n",
    "        if len(i) > 0:\n",
    "            ax[k].hist(i, bins=50)\n",
    "            ax[k].set_xlabel('Client')\n",
    "            ax[k].set_ylabel('Num errors')\n",
    "            ax[k].set_title('Iteration {}\\nwith acc:{:.3f}'.format(-j + iter_max, test_acc[-j + iter_max]))\n",
    "            k += 1\n",
    "        if k == 5:\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST MULTIPLE WEIGHTS WITH CLIPED IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 163.044 Accuracy: 0.742 Test Loss: 175.178 Accuracy: 0.164\n",
      "Average time per epoch 3.265s +- 0.139\n",
      "Max accuracy of 0.183 achieved at epoch 17\n",
      "Epoch 50. Train Loss: 162.021 Accuracy: 0.743 Test Loss: 175.926 Accuracy: 0.161\n",
      "Average time per epoch 3.362s +- 0.118\n",
      "Max accuracy of 0.168 achieved at epoch 22\n",
      "Epoch 50. Train Loss: 159.568 Accuracy: 0.752 Test Loss: 176.468 Accuracy: 0.162\n",
      "Average time per epoch 3.323s +- 0.062\n",
      "Max accuracy of 0.172 achieved at epoch 14\n",
      "Epoch 50. Train Loss: 164.002 Accuracy: 0.740 Test Loss: 176.916 Accuracy: 0.159\n",
      "Average time per epoch 3.322s +- 0.091\n",
      "Max accuracy of 0.171 achieved at epoch 13\n",
      "Epoch 50. Train Loss: 163.388 Accuracy: 0.733 Test Loss: 174.974 Accuracy: 0.163\n",
      "Average time per epoch 3.337s +- 0.103\n",
      "Max accuracy of 0.185 achieved at epoch 15\n",
      "Average accuracy 0.176 +- 0.007. Av loss 173.102\n",
      "Epoch 50. Train Loss: 90.616 Accuracy: 0.731 Test Loss: 175.381 Accuracy: 0.1551\n",
      "Average time per epoch 3.333s +- 0.058\n",
      "Max accuracy of 0.172 achieved at epoch 19\n",
      "Epoch 50. Train Loss: 89.856 Accuracy: 0.732 Test Loss: 174.938 Accuracy: 0.1654\n",
      "Average time per epoch 3.332s +- 0.066\n",
      "Max accuracy of 0.178 achieved at epoch 16\n",
      "Epoch 50. Train Loss: 88.657 Accuracy: 0.743 Test Loss: 174.357 Accuracy: 0.1666\n",
      "Average time per epoch 3.328s +- 0.048\n",
      "Max accuracy of 0.174 achieved at epoch 17\n",
      "Epoch 50. Train Loss: 90.251 Accuracy: 0.735 Test Loss: 175.464 Accuracy: 0.1575\n",
      "Average time per epoch 3.319s +- 0.062\n",
      "Max accuracy of 0.184 achieved at epoch 18\n",
      "Epoch 50. Train Loss: 89.765 Accuracy: 0.732 Test Loss: 175.652 Accuracy: 0.1573\n",
      "Average time per epoch 3.335s +- 0.049\n",
      "Max accuracy of 0.183 achieved at epoch 18\n",
      "Average accuracy 0.178 +- 0.005. Av loss 171.948\n",
      "Epoch 50. Train Loss: 5.347 Accuracy: 0.517 Test Loss: 168.053 Accuracy: 0.197\n",
      "Average time per epoch 3.329s +- 0.051\n",
      "Max accuracy of 0.200 achieved at epoch 35\n",
      "Epoch 50. Train Loss: 5.216 Accuracy: 0.539 Test Loss: 170.208 Accuracy: 0.181\n",
      "Average time per epoch 3.326s +- 0.070\n",
      "Max accuracy of 0.192 achieved at epoch 24\n",
      "Epoch 50. Train Loss: 5.214 Accuracy: 0.532 Test Loss: 169.385 Accuracy: 0.188\n",
      "Average time per epoch 3.338s +- 0.063\n",
      "Max accuracy of 0.201 achieved at epoch 26\n",
      "Epoch 50. Train Loss: 5.396 Accuracy: 0.543 Test Loss: 168.971 Accuracy: 0.194\n",
      "Average time per epoch 3.339s +- 0.055\n",
      "Max accuracy of 0.205 achieved at epoch 22\n",
      "Epoch 50. Train Loss: 5.284 Accuracy: 0.522 Test Loss: 169.500 Accuracy: 0.193\n",
      "Average time per epoch 3.335s +- 0.053\n",
      "Max accuracy of 0.203 achieved at epoch 26\n",
      "Average accuracy 0.200 +- 0.004. Av loss 167.830\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "for w in [0.05, 0.5, 0.99]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(20, weighted=w)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}'.format(np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc, total_loss = [], []\n",
    "for im_size_ in [30, 50, 70]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size_, weighted=0.999)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}'.format(np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "tables = list(range(1,95))\n",
    "random.shuffle(tables)\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.85)], tables[int(len(tables)*0.85):]\n",
    "\n",
    "# model parameters\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=50\n",
    "\n",
    "im_size = 40\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "model = ImagesP3(31, MAX_V, im_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion2 = nn.CrossEntropyLoss() \n",
    "criterion1 = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "for epoch in range(n_epochs):\n",
    "    current_time = time.time()\n",
    "    train_l, accuracy, test_l, test_a, idx_failures, output = \\\n",
    "    epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size=im_size, simple=True)\n",
    "    times.append(time.time() - current_time)\n",
    "    loss.append(train_l)\n",
    "    test_loss.append(test_l)\n",
    "    acc.append(accuracy)\n",
    "    test_acc.append(test_a)\n",
    "    idx_f.append(idx_failures)\n",
    "    \n",
    "print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "max_acc = np.max(test_acc)\n",
    "iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(20,10))\n",
    "ax[0].plot(loss)\n",
    "ax[0].plot(test_loss)\n",
    "ax[1].plot(acc)\n",
    "ax[1].plot(test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize=(15,4))\n",
    "k = 0\n",
    "for i in idx_f[::-1]:\n",
    "    if len(i) > 0:\n",
    "        ax[k].hist(i, bins=50)\n",
    "        k += 1\n",
    "    if k == 5:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].detach()\n",
    "a = nn.Softmax()\n",
    "_, jj = torch.max(output, dim=1)\n",
    "_, j = torch.max(a(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j == jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(test_tables, model):\n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    idx_failures = []\n",
    "\n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./relative_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./relative_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "\n",
    "            x = np.asarray([np.hstack(data[i]) for i in t_idx])\n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "\n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "\n",
    "            output = model(test_x)\n",
    "            batch_loss = criterion(output, test_y)\n",
    "\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output,1)\n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "                \n",
    "    return test_loss, test_acc, idx_failures, output, test_y, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 10\n",
    "idx = list(range(1,len(data)))\n",
    "for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            x = np.asarray([np.hstack(data[i][0]) for i in t_idx])\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            x_padded = np.zeros((len(t_idx), inp_size))\n",
    "            for i in range(len(t_idx)):\n",
    "                x_padded[i, :len(x[i])] = x[i]\n",
    "            \n",
    "            train_x = torch.tensor(x_padded).type(torch.FloatTensor)\n",
    "            \n",
    "            #train_y = torch.zeros(mini_batch_size, MAX_V).type(torch.LongTensor)\n",
    "            #train_y[range(mini_batch_size), y] = 1\n",
    "\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            output = model(train_x)\n",
    "            \n",
    "            _, a = torch.max(output,1)\n",
    "            acc.append((train_y == a).sum())\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./minmax_data/data_vector_{}.npy'.format(1), allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(30).reshape(30,1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_idx = list(range(3,7))\n",
    "x = np.asarray([np.hstack(data[i]) for i in t_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(np.asarray(data[3][1:])).type(torch.FloatTensor)\n",
    "d = torch.tensor(np.asarray(data[3][:1])).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.cat([b,c], dim=1).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.view(30,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = torch.cat([d,r], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e,i in enumerate(train_y):\n",
    "    x = train_x[e]\n",
    "    inp = x[:4]\n",
    "    cars = x[4:]\n",
    "    c = a[e]\n",
    "    if i != c:\n",
    "        print('real', i)\n",
    "        print(cars[i*6:i*6+6])\n",
    "        print('pred', c)\n",
    "        print(cars[c*6:c*6+6])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
