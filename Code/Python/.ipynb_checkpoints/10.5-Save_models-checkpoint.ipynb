{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.5 Training of different models and save weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the training of different models to generate their final weights. Deeper analysis of each architecture is provided in notebooks 7.x where different parameters are studied in each case. The comparison of the models in terms of accuracy and training time can be performed exercuting the script `training_models.py`. Here we use those models that have been more promising or are interesting for the sake of comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import optim\n",
    "from models import Net_Final, Net_Final_BIG, ImagesP2, ImagesP4\n",
    "import training_models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(285558)\n",
    "global MAX_V, mini_batch_size, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "\n",
    "tables = list(range(1,98))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.9)], tables[int(len(tables)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 13, 14, 16, 17, 20, 23, 26, 28, 36, 38, 40, 41, 42, 45, 49, 51, 52, 53, 54, 57, 62, 67, 68, 69, 78, 83, 86, 96]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(test_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE 4. Double Input Double Output MODEL 3. IMAGE SIZE = 30\n",
      "\n",
      "Epoch 15. Train Loss: 5.264 Accuracy: 0.565 Test Loss: 4.629 Accuracy: 0.243\n",
      "Average time per epoch 11.628s +- 0.510\n",
      "Max accuracy of 0.244 achieved at epoch 12\n",
      "Epoch 15. Train Loss: 5.071 Accuracy: 0.578 Test Loss: 4.709 Accuracy: 0.243\n",
      "Average time per epoch 11.867s +- 0.153\n",
      "Max accuracy of 0.244 achieved at epoch 12\n",
      "Epoch 15. Train Loss: 4.097 Accuracy: 0.567 Test Loss: 4.867 Accuracy: 0.236\n",
      "Average time per epoch 11.819s +- 0.122\n",
      "Max accuracy of 0.236 achieved at epoch 14\n",
      "Epoch 15. Train Loss: 4.686 Accuracy: 0.574 Test Loss: 5.478 Accuracy: 0.249\n",
      "Average time per epoch 11.860s +- 0.092\n",
      "Max accuracy of 0.249 achieved at epoch 14\n",
      "Average accuracy 0.243 +- 0.005. Av loss 4.273\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n",
      "\n",
      "CASE 5. Double Input Double Output MODEL 3. IMAGE SIZE = 50\n",
      "\n",
      "Epoch 15. Train Loss: 6.285 Accuracy: 0.610 Test Loss: 6.179 Accuracy: 0.252\n",
      "Average time per epoch 29.298s +- 0.143\n",
      "Max accuracy of 0.252 achieved at epoch 14\n",
      "Epoch 15. Train Loss: 5.675 Accuracy: 0.618 Test Loss: 6.679 Accuracy: 0.250\n",
      "Average time per epoch 29.358s +- 0.166\n",
      "Max accuracy of 0.251 achieved at epoch 13\n",
      "Epoch 15. Train Loss: 5.660 Accuracy: 0.611 Test Loss: 6.890 Accuracy: 0.247\n",
      "Average time per epoch 29.358s +- 0.155\n",
      "Max accuracy of 0.247 achieved at epoch 14\n",
      "Epoch 15. Train Loss: 6.232 Accuracy: 0.609 Test Loss: 6.727 Accuracy: 0.243\n",
      "Average time per epoch 29.414s +- 0.424\n",
      "Max accuracy of 0.243 achieved at epoch 14\n",
      "Average accuracy 0.248 +- 0.004. Av loss 6.083\n",
      " -------------\n",
      "Num parameters: 1153820\t Num Trainable parameters: 1153820\n",
      "\n",
      "CASE 6. Double Input Double Output MODEL 4. IMAGE SIZE = 30. 2 channels\n",
      "\n",
      "Epoch 15. Train Loss: 7.450 Accuracy: 0.589 Test Loss: 6.751 Accuracy: 0.244\n",
      "Average time per epoch 7.013s +- 0.126\n",
      "Max accuracy of 0.244 achieved at epoch 13\n",
      "Epoch 15. Train Loss: 7.777 Accuracy: 0.587 Test Loss: 7.375 Accuracy: 0.242\n",
      "Average time per epoch 6.982s +- 0.073\n",
      "Max accuracy of 0.242 achieved at epoch 14\n",
      "Epoch 15. Train Loss: 7.403 Accuracy: 0.593 Test Loss: 6.593 Accuracy: 0.243\n",
      "Average time per epoch 6.970s +- 0.111\n",
      "Max accuracy of 0.243 achieved at epoch 14\n",
      "Epoch 15. Train Loss: 7.841 Accuracy: 0.573 Test Loss: 6.516 Accuracy: 0.242\n",
      "Average time per epoch 7.012s +- 0.135\n",
      "Max accuracy of 0.243 achieved at epoch 13\n",
      "Average accuracy 0.243 +- 0.001. Av loss 6.201\n",
      " -------------\n",
      "Num parameters: 434435\t Num Trainable parameters: 434435\n",
      "\n",
      "CASE 7. Double Input Double Output MODEL 4. IMAGE SIZE = 50. 2 channels\n",
      "\n",
      "Epoch 15. Train Loss: 6.533 Accuracy: 0.611 Test Loss: 6.827 Accuracy: 0.237\n",
      "Average time per epoch 15.789s +- 0.236\n",
      "Max accuracy of 0.242 achieved at epoch 13\n",
      "Epoch 15. Train Loss: 6.376 Accuracy: 0.616 Test Loss: 6.611 Accuracy: 0.245\n",
      "Average time per epoch 15.768s +- 0.223\n",
      "Max accuracy of 0.246 achieved at epoch 10\n",
      "Epoch 15. Train Loss: 6.116 Accuracy: 0.612 Test Loss: 7.025 Accuracy: 0.243\n",
      "Average time per epoch 15.778s +- 0.282\n",
      "Max accuracy of 0.243 achieved at epoch 14\n",
      "Epoch 15. Train Loss: 5.606 Accuracy: 0.631 Test Loss: 6.892 Accuracy: 0.235\n",
      "Average time per epoch 15.818s +- 0.190\n",
      "Max accuracy of 0.235 achieved at epoch 14\n",
      "Average accuracy 0.242 +- 0.004. Av loss 6.154\n",
      " -------------\n",
      "Num parameters: 1152515\t Num Trainable parameters: 1152515\n"
     ]
    }
   ],
   "source": [
    "%run training_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train_single_inp_single_out(model, train_table, test_tables, optimizer, criterion1, \\\n",
    "                e, im_size, clipped):\n",
    "    \n",
    "\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "           \n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                if clipped:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            \n",
    "            output2 = model(train_x)\n",
    "            batch_loss = criterion1(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    #model.eval()\n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "           \n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                if clipped:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            output2 = model(test_x)\n",
    "            batch_loss = criterion1(output2, test_y)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train_single_input_double_output(model, train_table, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                e, im_size, weighted, clipped):\n",
    "    \n",
    "\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                if clipped:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x)\n",
    "            batch_loss = 100*weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "                \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    #model.eval()\n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                if clipped:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x)\n",
    "            batch_loss = 100*weighted*criterion1(output1, test_y_aux) + (1- weighted)*criterion2(output2, test_y)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                e, im_size, weighted, clipped):\n",
    "    \n",
    "\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            if clipped:\n",
    "                x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            else:\n",
    "                x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                if clipped:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            batch_loss = 100*weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    #model.eval()\n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            if clipped:\n",
    "                x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            else:\n",
    "                x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                if clipped:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            batch_loss = 100*weighted*criterion1(output1, test_y_aux) + (1-weighted)*criterion2(output2, test_y)\n",
    "\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(model, im_size, n_epochs, simple=False, weighted=0.5, clipped=False, single_inp=False):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss()\n",
    "        \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        if single_inp and simple:\n",
    "            train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "            epoch_train_single_inp_single_out(model, train_tables, test_tables, optimizer, criterion2, \\\n",
    "                epoch, im_size, clipped)\n",
    "        elif single_inp and not simple:\n",
    "            train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "            epoch_train_single_input_double_output(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                epoch, im_size, weighted, clipped)\n",
    "        else:\n",
    "            train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "            epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                    epoch, im_size, weighted, clipped)\n",
    "            \n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1. Single Input - Single Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 210.501 Accuracy: 0.377 Test Loss: 148.023 Accuracy: 0.129\n",
      "Average time per epoch 11.233s +- 1.699\n",
      "Max accuracy of 0.131 achieved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ImagesP2. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "im_size = 30\n",
    "\n",
    "model1 = ImagesP2(31, MAX_V, im_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model1, im_size, n_epochs, simple=True, clipped=False, single_inp=True)\n",
    "torch.save(model1, 'model_weights2/model1_evaluation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Train Loss: 593.330 Accuracy: 0.225 Test Loss: 350.094 Accuracy: 0.121\n",
      "Average time per epoch 11.433s +- 1.128\n",
      "Max accuracy of 0.122 achieved at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ImagesP4. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "im_size = 30\n",
    "\n",
    "model2 = ImagesP4(31, MAX_V, im_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model2, im_size, n_epochs, weighted=0.999, clipped=False, single_inp=True)\n",
    "torch.save(model2, 'model_weights2/model2_evaluation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 386.647 Accuracy: 0.323 Test Loss: 268.233 Accuracy: 0.135\n",
      "Average time per epoch 11.042s +- 0.114\n",
      "Max accuracy of 0.135 achieved at epoch 7\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "im_size = 30\n",
    "\n",
    "model3 = ImagesP4(31, MAX_V, im_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model2, im_size, n_epochs, weighted=0.5, clipped=False, single_inp=True)\n",
    "torch.save(model3, 'model_weights2/model3_evaluation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 442.326 Accuracy: 0.421 Test Loss: 388.499 Accuracy: 0.195\n",
      "Average time per epoch 10.356s +- 0.452\n",
      "Max accuracy of 0.198 achieved at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Net_Final. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "im_size = 30\n",
    "\n",
    "model4 = Net_Final(31, MAX_V, im_size, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model4, im_size, n_epochs, weighted=0.999, clipped=False)\n",
    "torch.save(model4, 'model_weights2/model4_evaluation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 472.155 Accuracy: 0.428 Test Loss: 364.644 Accuracy: 0.202\n",
      "Average time per epoch 25.312s +- 1.017\n",
      "Max accuracy of 0.204 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "im_size = 50\n",
    "\n",
    "model5 = Net_Final(31, MAX_V, im_size, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model5, im_size, n_epochs, weighted=0.999, clipped=False)\n",
    "torch.save(model5, 'model_weights2/model5_evaluation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 535.794 Accuracy: 0.424 Test Loss: 386.773 Accuracy: 0.205\n",
      "Average time per epoch 6.341s +- 0.250\n",
      "Max accuracy of 0.205 achieved at epoch 7\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "im_size = 30\n",
    "\n",
    "model6 = Net_Final(2, MAX_V, im_size, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model6, im_size, n_epochs, weighted=0.999, clipped=True)\n",
    "torch.save(model6, 'model_weights2/model6_evaluation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 470.826 Accuracy: 0.427 Test Loss: 500.189 Accuracy: 0.204\n",
      "Average time per epoch 13.930s +- 0.535\n",
      "Max accuracy of 0.207 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "im_size = 50\n",
    "\n",
    "model7 = Net_Final(2, MAX_V, im_size, 3)\n",
    "loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "    evaluating_model(model7, im_size, n_epochs, weighted=0.999, clipped=True)\n",
    "torch.save(model7, 'model_weights2/model7_evaluation.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
