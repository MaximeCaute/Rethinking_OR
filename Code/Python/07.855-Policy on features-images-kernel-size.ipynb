{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  07.8 - Learning Policy on Image + Attributes - loss 2\n",
    "\n",
    "Fully connected network with:\n",
    "- Input: \n",
    "    - **184** size vector with:\n",
    "        * client information: [current latitude, current longitud, destination latitude, destination longitude]\n",
    "        * vehicles information: [load, queue, current latitude, current longitud, next latitude, next longitude]\n",
    "    - Image with 31 channel: \n",
    "        * Channel 0 --> client representation. its current location is marked as 1; its destination as -1\n",
    "        * Channels > 0 --> vehicle current location (1)\n",
    "        \n",
    "- Output: One hot encoding vector with 30 entries representing the available vehicles\n",
    "\n",
    "\n",
    "The attribute input is merged later with a partial output of the network\n",
    "\n",
    "loss 1: vehicle assignment\n",
    "loss 2: vehicle position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesP3(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(2, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 4*(self.cs-1))*(im_size - 4*(self.cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 300),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(300, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv4(self.conv3(x2))\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 4*(self.cs-1))*(x.shape[-1] - 4*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size=30, simple=False, weighted=0.5):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = criterion1(output1, test_y_aux) + criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4915)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=40\n",
    "\n",
    "tables = list(range(1,90))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "            tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.9)], \\\n",
    "            tables[int(len(tables)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImagesP3(80, MAX_V, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total num of parameters: {}'.format(sum(p.numel() for p in model.parameters())))\n",
    "print('Num of trainable parameters: {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2\n",
    "\n",
    "\n",
    "def visualize_results(loss, test_loss, acc, test_acc, idx_f):\n",
    "    f, ax = plt.subplots(2, 1, figsize=(20,8))\n",
    "    ax[0].plot(loss)\n",
    "    ax[0].plot(test_loss)\n",
    "    ax[1].plot(acc)\n",
    "    ax[1].plot(test_acc)\n",
    "    plt.show()\n",
    "    \n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc) \n",
    "    iter_max = iter_max[0][0]\n",
    "\n",
    "    fig, ax = plt.subplots(1,5, figsize=(20,4))\n",
    "    k = 0\n",
    "    idx = idx_f[:iter_max+1]\n",
    "    for j, i in enumerate(idx[::-1]):\n",
    "        if len(i) > 0:\n",
    "            ax[k].hist(i, bins=50)\n",
    "            ax[k].set_xlabel('Client')\n",
    "            ax[k].set_ylabel('Num errors')\n",
    "            ax[k].set_title('Iteration {}\\nwith acc:{:.3f}'.format(-j + iter_max, test_acc[-j + iter_max]))\n",
    "            k += 1\n",
    "        if k == 5:\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 3.959 Accuracy: 0.532 Test Loss: 169.891 Accuracy: 0.187\n",
      "Average time per epoch 11.082s +- 1.950\n",
      "Max accuracy of 0.201 achieved at epoch 22\n",
      "Epoch 40. Train Loss: 4.084 Accuracy: 0.552 Test Loss: 169.746 Accuracy: 0.191\n",
      "Average time per epoch 18.875s +- 59.310\n",
      "Max accuracy of 0.201 achieved at epoch 24\n",
      "Epoch 40. Train Loss: 4.595 Accuracy: 0.533 Test Loss: 167.956 Accuracy: 0.194\n",
      "Average time per epoch 10.735s +- 0.168\n",
      "Max accuracy of 0.204 achieved at epoch 25\n",
      "Epoch 40. Train Loss: 4.018 Accuracy: 0.532 Test Loss: 169.715 Accuracy: 0.181\n",
      "Average time per epoch 10.723s +- 0.136\n",
      "Max accuracy of 0.194 achieved at epoch 28\n",
      "Epoch 40. Train Loss: 3.982 Accuracy: 0.541 Test Loss: 170.716 Accuracy: 0.181\n",
      "Average time per epoch 10.802s +- 0.190\n",
      "Max accuracy of 0.193 achieved at epoch 31\n",
      "Average accuracy 0.199 +- 0.004. Av loss 168.667\n",
      "Epoch 40. Train Loss: 4.348 Accuracy: 0.509 Test Loss: 169.576 Accuracy: 0.196\n",
      "Average time per epoch 12.163s +- 0.147\n",
      "Max accuracy of 0.203 achieved at epoch 35\n",
      "Epoch 40. Train Loss: 4.295 Accuracy: 0.512 Test Loss: 169.976 Accuracy: 0.195\n",
      "Average time per epoch 12.169s +- 0.143\n",
      "Max accuracy of 0.201 achieved at epoch 27\n",
      "Epoch 40. Train Loss: 4.629 Accuracy: 0.503 Test Loss: 167.554 Accuracy: 0.192\n",
      "Average time per epoch 12.179s +- 0.311\n",
      "Max accuracy of 0.195 achieved at epoch 31\n",
      "Epoch 40. Train Loss: 4.217 Accuracy: 0.528 Test Loss: 169.510 Accuracy: 0.181\n",
      "Average time per epoch 12.197s +- 0.194\n",
      "Max accuracy of 0.195 achieved at epoch 26\n",
      "Epoch 40. Train Loss: 5.216 Accuracy: 0.510 Test Loss: 170.720 Accuracy: 0.180\n",
      "Average time per epoch 12.219s +- 0.139\n",
      "Max accuracy of 0.188 achieved at epoch 23\n",
      "Average accuracy 0.196 +- 0.005. Av loss 169.076\n",
      "Epoch 40. Train Loss: 4.178 Accuracy: 0.486 Test Loss: 169.399 Accuracy: 0.198\n",
      "Average time per epoch 16.583s +- 0.226\n",
      "Max accuracy of 0.209 achieved at epoch 30\n",
      "Epoch 40. Train Loss: 4.050 Accuracy: 0.502 Test Loss: 170.121 Accuracy: 0.192\n",
      "Average time per epoch 16.654s +- 0.175\n",
      "Max accuracy of 0.203 achieved at epoch 34\n",
      "Epoch 40. Train Loss: 4.128 Accuracy: 0.487 Test Loss: 171.225 Accuracy: 0.186\n",
      "Average time per epoch 16.653s +- 0.187\n",
      "Max accuracy of 0.192 achieved at epoch 32\n",
      "Epoch 40. Train Loss: 3.950 Accuracy: 0.491 Test Loss: 170.007 Accuracy: 0.192\n",
      "Average time per epoch 16.678s +- 0.200\n",
      "Max accuracy of 0.196 achieved at epoch 30\n",
      "Epoch 40. Train Loss: 4.114 Accuracy: 0.480 Test Loss: 169.672 Accuracy: 0.195\n",
      "Average time per epoch 16.649s +- 0.178\n",
      "Max accuracy of 0.200 achieved at epoch 22\n",
      "Average accuracy 0.200 +- 0.006. Av loss 169.463\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3, 5, 10]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}'.format(np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of Batch Norm in Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 4.933 Accuracy: 0.171 Test Loss: 169.134 Accuracy: 0.125\n",
      "Average time per epoch 11.069s +- 0.641\n",
      "Max accuracy of 0.125 achieved at epoch 39\n",
      "Epoch 40. Train Loss: 4.527 Accuracy: 0.179 Test Loss: 148.015 Accuracy: 0.137\n",
      "Average time per epoch 11.087s +- 0.417\n",
      "Max accuracy of 0.138 achieved at epoch 37\n",
      "Epoch 40. Train Loss: 4.519 Accuracy: 0.176 Test Loss: 164.081 Accuracy: 0.132\n",
      "Average time per epoch 11.139s +- 0.294\n",
      "Max accuracy of 0.133 achieved at epoch 38\n",
      "Epoch 40. Train Loss: 5.202 Accuracy: 0.189 Test Loss: 164.299 Accuracy: 0.132\n",
      "Average time per epoch 11.195s +- 0.414\n",
      "Max accuracy of 0.133 achieved at epoch 38\n",
      "Epoch 40. Train Loss: 5.327 Accuracy: 0.182 Test Loss: 170.301 Accuracy: 0.131\n",
      "Average time per epoch 11.124s +- 0.224\n",
      "Max accuracy of 0.131 achieved at epoch 39\n",
      "Average accuracy 0.132 +- 0.004. Av loss 163.075\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}'.format(np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of layer size -- from 100 to 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 3.907 Accuracy: 0.768 Test Loss: 172.820 Accuracy: 0.183\n",
      "Average time per epoch 11.987s +- 0.659\n",
      "Max accuracy of 0.213 achieved at epoch 11\n",
      "Epoch 40. Train Loss: 3.300 Accuracy: 0.754 Test Loss: 174.079 Accuracy: 0.180\n",
      "Average time per epoch 11.970s +- 0.357\n",
      "Max accuracy of 0.201 achieved at epoch 17\n",
      "Epoch 40. Train Loss: 3.469 Accuracy: 0.777 Test Loss: 174.829 Accuracy: 0.170\n",
      "Average time per epoch 12.012s +- 0.169\n",
      "Max accuracy of 0.195 achieved at epoch 7\n",
      "Epoch 40. Train Loss: 3.527 Accuracy: 0.774 Test Loss: 173.471 Accuracy: 0.185\n",
      "Average time per epoch 12.136s +- 0.481\n",
      "Max accuracy of 0.209 achieved at epoch 11\n",
      "Epoch 40. Train Loss: 3.852 Accuracy: 0.767 Test Loss: 173.270 Accuracy: 0.182\n",
      "Average time per epoch 12.275s +- 0.470\n",
      "Max accuracy of 0.199 achieved at epoch 17\n",
      "Average accuracy 0.204 +- 0.006. Av loss 169.756\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImagesP3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(31, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(10, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=264, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=264, out_features=30, bias=True)\n",
      "  )\n",
      "  (f_aux): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=2, bias=True)\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=210, out_features=300, bias=True)\n",
      "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc4): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=30, bias=True)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 3.457 Accuracy: 0.776 Test Loss: 173.242 Accuracy: 0.179\n",
      "Average time per epoch 16.937s +- 1.201\n",
      "Max accuracy of 0.196 achieved at epoch 25\n",
      "Epoch 40. Train Loss: 3.197 Accuracy: 0.774 Test Loss: 173.496 Accuracy: 0.173\n",
      "Average time per epoch 17.093s +- 0.576\n",
      "Max accuracy of 0.196 achieved at epoch 10\n",
      "Epoch 40. Train Loss: 3.176 Accuracy: 0.775 Test Loss: 174.008 Accuracy: 0.177\n",
      "Average time per epoch 16.982s +- 0.796\n",
      "Max accuracy of 0.198 achieved at epoch 9\n",
      "Epoch 40. Train Loss: 3.253 Accuracy: 0.788 Test Loss: 174.502 Accuracy: 0.173\n",
      "Average time per epoch 17.325s +- 0.417\n",
      "Max accuracy of 0.207 achieved at epoch 14\n",
      "Epoch 40. Train Loss: 3.262 Accuracy: 0.770 Test Loss: 174.042 Accuracy: 0.172\n",
      "Average time per epoch 17.365s +- 0.626\n",
      "Max accuracy of 0.200 achieved at epoch 17\n",
      "Average accuracy 0.199 +- 0.004. Av loss 169.389\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImagesP3(31, MAX_V, im_size, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImagesP3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(31, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(5, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=968, out_features=264, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=264, out_features=30, bias=True)\n",
      "  )\n",
      "  (f_aux): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=2, bias=True)\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=210, out_features=300, bias=True)\n",
      "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc4): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=30, bias=True)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 4.649 Accuracy: 0.760 Test Loss: 173.698 Accuracy: 0.170\n",
      "Average time per epoch 10.527s +- 0.880\n",
      "Max accuracy of 0.202 achieved at epoch 10\n",
      "Epoch 40. Train Loss: 4.082 Accuracy: 0.751 Test Loss: 174.303 Accuracy: 0.175\n",
      "Average time per epoch 10.753s +- 0.222\n",
      "Max accuracy of 0.198 achieved at epoch 12\n",
      "Epoch 40. Train Loss: 3.410 Accuracy: 0.770 Test Loss: 173.768 Accuracy: 0.169\n",
      "Average time per epoch 10.711s +- 0.361\n",
      "Max accuracy of 0.203 achieved at epoch 11\n",
      "Epoch 40. Train Loss: 3.789 Accuracy: 0.757 Test Loss: 171.505 Accuracy: 0.181\n",
      "Average time per epoch 10.452s +- 0.495\n",
      "Max accuracy of 0.199 achieved at epoch 9\n",
      "Epoch 40. Train Loss: 3.455 Accuracy: 0.758 Test Loss: 173.542 Accuracy: 0.177\n",
      "Average time per epoch 10.694s +- 0.483\n",
      "Max accuracy of 0.197 achieved at epoch 12\n",
      "Average accuracy 0.200 +- 0.002. Av loss 169.241\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesP3(31, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUTURE LOCATION AS -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesVal2(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))*(im_size - 3*(self.cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 300),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(300, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1]\n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train2(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size=30, simple=False, weighted=0.5):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = criterion1(output1, test_y_aux) + criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "\n",
    "def evaluating_model(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train2(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 3.061 Accuracy: 0.771 Test Loss: 172.059 Accuracy: 0.187\n",
      "Average time per epoch 12.004s +- 0.349\n",
      "Max accuracy of 0.213 achieved at epoch 17\n",
      "Epoch 40. Train Loss: 4.349 Accuracy: 0.758 Test Loss: 173.645 Accuracy: 0.178\n",
      "Average time per epoch 11.686s +- 0.225\n",
      "Max accuracy of 0.201 achieved at epoch 10\n",
      "Epoch 40. Train Loss: 3.505 Accuracy: 0.767 Test Loss: 174.196 Accuracy: 0.163\n",
      "Average time per epoch 11.456s +- 0.136\n",
      "Max accuracy of 0.194 achieved at epoch 10\n",
      "Epoch 40. Train Loss: 3.483 Accuracy: 0.761 Test Loss: 174.070 Accuracy: 0.177\n",
      "Average time per epoch 11.472s +- 0.146\n",
      "Max accuracy of 0.194 achieved at epoch 16\n",
      "Epoch 40. Train Loss: 3.452 Accuracy: 0.774 Test Loss: 174.474 Accuracy: 0.173\n",
      "Average time per epoch 11.458s +- 0.119\n",
      "Max accuracy of 0.198 achieved at epoch 13\n",
      "Average accuracy 0.200 +- 0.007. Av loss 169.679\n",
      " -------------\n"
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesVal2(MAX_V, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train3(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size=30, simple=False, weighted=0.5):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), 2*nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][2*i-1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][2*i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "           \n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            train_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            output1, output2 = model(train_x, train_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, train_y)\n",
    "            else:\n",
    "                batch_loss = weighted*criterion1(output1, train_y_aux) + (1- weighted)*criterion2(output2, train_y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            x = np.zeros((len(t_idx), 2*nv+1, im_size, im_size))\n",
    "            loc_y = np.zeros((len(t_idx), 2))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                loc = int(data_y[cl])\n",
    "                loc_y[k] = [data[cl][loc+1][2], data[cl][loc+1][3]]\n",
    "                \n",
    "                x_aux.append(torch.tensor(np.asarray(data[cl][1:])).type(torch.FloatTensor))\n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "\n",
    "                for i in range(1,31):\n",
    "                    x[k][2*i-1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                    x[k][2*i][int(data[cl][i][4]//dist)][int(data[cl][i][5]//dist)] = -1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            test_x_aux = torch.stack(x_aux).type(torch.FloatTensor)\n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            test_y_aux = torch.tensor(loc_y).type(torch.FloatTensor)\n",
    "            \n",
    "            output1, output2 = model(test_x, test_x_aux)\n",
    "            if simple:\n",
    "                batch_loss = criterion2(output2, test_y)\n",
    "            else:\n",
    "                batch_loss = criterion1(output1, test_y_aux) + criterion2(output2, test_y)\n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output2,1)\n",
    "            \n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures, output2\n",
    "\n",
    "def evaluating_model3(im_size, model, simple=False, weighted=0.5):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss() \n",
    "    criterion1 = nn.MSELoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures, o2 = \\\n",
    "        epoch_train3(model, train_tables, test_tables, optimizer, criterion1, criterion2, epoch, im_size, simple, weighted)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 2.998 Accuracy: 0.774 Test Loss: 173.151 Accuracy: 0.165\n",
      "Average time per epoch 13.949s +- 0.576\n",
      "Max accuracy of 0.198 achieved at epoch 12\n",
      "Epoch 40. Train Loss: 3.928 Accuracy: 0.756 Test Loss: 173.804 Accuracy: 0.173\n",
      "Average time per epoch 14.244s +- 0.580\n",
      "Max accuracy of 0.202 achieved at epoch 11\n",
      "Epoch 19. Train Loss: 5.175 Accuracy: 0.538 Test Loss: 170.262 Accuracy: 0.194"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-fdddaf1f7033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagesVal2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mMAX_V\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluating_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d5e419268eee>\u001b[0m in \u001b[0;36mevaluating_model3\u001b[0;34m(im_size, model, simple, weighted)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mcurrent_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mtrain_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_failures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mepoch_train3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d5e419268eee>\u001b[0m in \u001b[0;36mepoch_train3\u001b[0;34m(model, train_table, test_tables, optimizer, criterion1, criterion2, e, im_size, simple, weighted)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_acc, total_loss = [], []\n",
    "im_size=30\n",
    "for kernel_size in [3]:\n",
    "    main_acc, main_loss = [], []\n",
    "    for trial in range(5):\n",
    "        model = ImagesVal2(2*MAX_V+1, MAX_V, im_size, kernel_size)\n",
    "        loss, acc, test_loss, test_acc, idx_f, times, o2 = evaluating_model3(im_size, model, weighted=0.99)\n",
    "        main_loss.append(min(test_loss))\n",
    "        main_acc.append(max(test_acc))\n",
    "    total_acc.append(main_acc)\n",
    "    total_loss.append(main_loss)\n",
    "    print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format(\\\n",
    "        np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
