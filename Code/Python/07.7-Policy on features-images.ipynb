{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  07.7 - Learning Policy on Image + Attributes\n",
    "\n",
    "Fully connected network with:\n",
    "- Input: \n",
    "    - **184** size vector with:\n",
    "        * client information: [current latitude, current longitud, destination latitude, destination longitude]\n",
    "        * vehicles information: [load, queue, current latitude, current longitud, next latitude, next longitude]\n",
    "    - Image with 31 channel: \n",
    "        * Channel 0 --> client representation. its current location is marked as 1; its destination as -1\n",
    "        * Channels > 0 --> vehicle current location (1)\n",
    "        \n",
    "- Output: One hot encoding vector with 30 entries representing the available vehicles\n",
    "\n",
    "\n",
    "The attribute input is merged later with a partial output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesP2(nn.Module):\n",
    "    def __init__(self, inp, im_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 10\n",
    "        self.cs = 5\n",
    "        self.im_size = im_size\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))*(im_size - 3*(self.cs-1)), 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, inp-1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "                \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = x3.view(-1,2*(self.im_size - 3*(self.cs-1))*(self.im_size - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_table, test_tables, optimizer, criterion, e, im_size=30, clipped=False):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1/(im_size-1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "\n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "        \n",
    "        \n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            \n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            if clipped:\n",
    "                x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            else:\n",
    "                x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "\n",
    "            \n",
    "            # iterate along clients\n",
    "            for k, cl in enumerate(t_idx):\n",
    "                \n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "                \n",
    "                if clipped:\n",
    "                    x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                # iterate along vehicles\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "\n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "\n",
    "            train_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            train_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute output\n",
    "            # output1, output2 = model(train_x, train_x_aux)\n",
    "            # batch_loss = criterion(output1, train_y) + criterion(output2, train_y)\n",
    "            output1= model(train_x)\n",
    "            batch_loss = criterion(output1, train_y)\n",
    "            \n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            _, a = torch.max(output1,1)\n",
    "            acc.append(float((train_y == a).sum())/len(train_y))\n",
    "            \n",
    "            \n",
    "            #acc.append(100*output.detach().max(1)[1].eq(train_y).sum().item()/(train_y.shape[0]*train_y.shape[1]*train_y.shape[2]))\n",
    "\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    for k,TABLE in enumerate(test_tables):\n",
    "        data = np.load('./minmax_data/data_vector_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        data_y = np.load('./minmax_data/data_vector_y_{}.npy'.format(TABLE), allow_pickle=True).tolist()\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        #random.shuffle(idx)\n",
    "        \n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b+mini_batch_size]\n",
    "            \n",
    "            if clipped:\n",
    "                x = np.zeros((len(t_idx), 2, im_size, im_size))\n",
    "            else:\n",
    "                x = np.zeros((len(t_idx), nv+1, im_size, im_size))\n",
    "            x_aux = []\n",
    "            \n",
    "            for k, cl in enumerate(t_idx):\n",
    "                \n",
    "                x[k][0][int(data[cl][0][0]//dist)][int(data[cl][0][1]//dist)] = 1\n",
    "                x[k][0][int(data[cl][0][2]//dist)][int(data[cl][0][3]//dist)] = -1\n",
    "                if clipped:\n",
    "                    x[k][1][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "                else:\n",
    "                    for i in range(1,31):\n",
    "                        x[k][i][int(data[cl][i][2]//dist)][int(data[cl][i][3]//dist)] = 1\n",
    "            \n",
    "            y = np.hstack([data_y[i] for i in t_idx])\n",
    "            \n",
    "            test_x = torch.tensor(x).type(torch.FloatTensor)\n",
    "            \n",
    "            test_y = torch.tensor(y).type(torch.LongTensor)\n",
    "            \n",
    "            output1= model(test_x)\n",
    "            batch_loss = criterion(output1, test_y) \n",
    "            \n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            _, a = torch.max(output1,1)\n",
    "            test_acc.append(float((test_y == a).sum())/len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "            \n",
    "            \n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e+1, sum_loss, np.mean(acc), test_loss,np.mean(test_acc)), end=\"\")\n",
    "    #print('Epoch {}. Test Loss: {:.3f}'.format(e+1,test_loss))\n",
    "    return sum_loss, np.sum(acc)/len(acc), test_loss, np.sum(test_acc)/len(test_acc), idx_failures\n",
    "    #return sum_loss, np.mean(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4915)\n",
    "global MAX_V, mini_batch_size, n_epochs, lr, inp_size\n",
    "MAX_V = 30\n",
    "mini_batch_size = 50\n",
    "n_epochs=30\n",
    "\n",
    "tables = list(range(1,90))\n",
    "random.shuffle(tables)\n",
    "\n",
    "lr = 0.0001\n",
    "inp_size = 4 + MAX_V*6\n",
    "\n",
    "train_tables, test_tables, validation_tables = \\\n",
    "tables[:int(len(tables)*0.6)], tables[int(len(tables)*0.6):int(len(tables)*0.85)], tables[int(len(tables)*0.85):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(im_size, model, n_epochs):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "        epoch_train(model, train_tables, test_tables, optimizer, criterion, epoch, im_size=im_size)\n",
    "        \n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2*np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "    \n",
    "    return loss, acc, test_loss, test_acc, idx_f, times\n",
    "\n",
    "\n",
    "def visualize_results(loss, test_loss, acc, test_acc, idx_f):\n",
    "    f, ax = plt.subplots(2, 1, figsize=(20,8))\n",
    "    ax[0].plot(loss)\n",
    "    ax[0].plot(test_loss)\n",
    "    ax[1].plot(acc)\n",
    "    ax[1].plot(test_acc)\n",
    "    plt.show()\n",
    "    \n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc ==  max_acc) \n",
    "    iter_max = iter_max[0][0]\n",
    "\n",
    "    fig, ax = plt.subplots(1,5, figsize=(20,4))\n",
    "    k = 0\n",
    "    idx = idx_f[:iter_max+1]\n",
    "    for j, i in enumerate(idx[::-1]):\n",
    "        if len(i) > 0:\n",
    "            ax[k].hist(i, bins=50)\n",
    "            ax[k].set_xlabel('Client')\n",
    "            ax[k].set_ylabel('Num errors')\n",
    "            ax[k].set_title('Iteration {}\\nwith acc:{:.3f}'.format(-j + iter_max, test_acc[-j + iter_max]))\n",
    "            k += 1\n",
    "        if k == 5:\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with image size = 30x30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30. Train Loss: 133.113 Accuracy: 0.635 Test Loss: 138.328 Accuracy: 0.136\n",
      "Average time per epoch 12.610s +- 0.951\n",
      "Max accuracy of 0.146 achieved at epoch 11\n",
      "Epoch 30. Train Loss: 131.406 Accuracy: 0.637 Test Loss: 141.776 Accuracy: 0.127\n",
      "Average time per epoch 12.748s +- 0.544\n",
      "Max accuracy of 0.149 achieved at epoch 11\n",
      "Epoch 30. Train Loss: 135.746 Accuracy: 0.623 Test Loss: 137.824 Accuracy: 0.138\n",
      "Average time per epoch 12.495s +- 0.490\n",
      "Max accuracy of 0.149 achieved at epoch 7\n",
      "Epoch 30. Train Loss: 131.459 Accuracy: 0.641 Test Loss: 142.067 Accuracy: 0.137\n",
      "Average time per epoch 12.562s +- 0.322\n",
      "Max accuracy of 0.145 achieved at epoch 12\n",
      "Epoch 30. Train Loss: 131.940 Accuracy: 0.642 Test Loss: 141.049 Accuracy: 0.137\n",
      "Average time per epoch 12.428s +- 0.417\n",
      "Max accuracy of 0.146 achieved at epoch 13\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    loss, acc, test_loss, test_acc, idx_f, times= evaluating_model(30)\n",
    "#visualize_results(loss, test_loss, acc, test_acc, idx_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11. Train Loss: 230.711 Accuracy: 0.319 Test Loss: 114.935 Accuracy: 0.138\n",
      "Average time per epoch 11.637s +- 0.424\n",
      "Max accuracy of 0.138 achieved at epoch 10\n"
     ]
    }
   ],
   "source": [
    "im_size = 30\n",
    "model30 = ImagesP2(31, im_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times= evaluating_model(30, model30, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ImagesP2. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/nataliebolon/miniconda3/envs/vehicles/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model30, 'model_simple_30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11. Train Loss: 176.716 Accuracy: 0.487 Test Loss: 124.270 Accuracy: 0.141\n",
      "Average time per epoch 10.905s +- 0.660\n",
      "Max accuracy of 0.142 achieved at epoch 5\n"
     ]
    }
   ],
   "source": [
    "im_size = 30\n",
    "model30_clipped = ImagesP2(2, im_size)\n",
    "loss, acc, test_loss, test_acc, idx_f, times= evaluating_model(30, model30, 11)\n",
    "torch.save(model30_clipped, 'model_simple_30_clipped.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model50, 'model_simple_30_clipped.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
